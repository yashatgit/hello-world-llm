{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3be4281-4650-4542-a5ea-76a7e8d64a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Jupyter Notebooks\n",
    "\n",
    "Interactive coding playground where you can write and run small chunks of code, see the results immediately, and even visualize data right next to your code. It's like a combination of a code editor, a terminal, and a document editor, all in one.\n",
    "\n",
    "[https://jupyter.org/install](https://jupyter.org/install)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f251772-f41b-4aa9-bc1a-d297806854ca",
   "metadata": {},
   "source": [
    "# LLM - Large Language Models\n",
    "- AI models focused on understanding, generating, and manipulating natural language. \n",
    "- Trained on large corpora of text data, enabling them to grasp the nuances of language, including syntax, semantics, and context. \n",
    "- Predict the probability of a sequence of words, which allows them to generate coherent and contextually relevant text based on given prompts.\n",
    "\n",
    "<img src=\"ss_1.jpeg\"/>\n",
    "\n",
    "Src: [Intro to Large Language Models - Andrej Karpathy](https://www.youtube.com/watch?v=zjkBMFhNj_g)\n",
    "\n",
    "[https://twitter.com/karpathy](https://twitter.com/karpathy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1001f3-7dd7-44f6-b4d4-0d23ac112a49",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "- Concepts of LLM application development\n",
    "    - Hello World\n",
    "    - Message roles and models\n",
    "    - Hallucinations\n",
    "    - RAG\n",
    "    - Function Calling\n",
    "    - Agents and Tools\n",
    "    - Frontend Copilot\n",
    "    - FAQ: Embeddings\n",
    "- Spaceweb Forge Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea1d58-b2f0-44b0-a0d9-1da820d6be5a",
   "metadata": {},
   "source": [
    "## Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dfc2855-87fe-4caa-932d-10b13e2c02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv from \"npm:dotenv/config\";\n",
    "import {OpenAI} from \"npm:openai\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "801eaee1-9d29-416b-8373-def01a01673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// file: .env\n",
    "// OPENAI_API_KEY = \"....\"\n",
    "\n",
    "const _OpenAI = new OpenAI();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf01f89d-9fb5-4441-85d9-bfad848547df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  id: \u001b[32m\"chatcmpl-A6GJlHxN8B8ZogalIoVejuU7kFFw7\"\u001b[39m,\n",
      "  object: \u001b[32m\"chat.completion\"\u001b[39m,\n",
      "  created: \u001b[33m1726056057\u001b[39m,\n",
      "  model: \u001b[32m\"gpt-3.5-turbo-0125\"\u001b[39m,\n",
      "  choices: [\n",
      "    {\n",
      "      index: \u001b[33m0\u001b[39m,\n",
      "      message: {\n",
      "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
      "        content: \u001b[32m\"Hello! How are you today?\"\u001b[39m,\n",
      "        refusal: \u001b[1mnull\u001b[22m\n",
      "      },\n",
      "      logprobs: \u001b[1mnull\u001b[22m,\n",
      "      finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
      "    }\n",
      "  ],\n",
      "  usage: { prompt_tokens: \u001b[33m9\u001b[39m, completion_tokens: \u001b[33m7\u001b[39m, total_tokens: \u001b[33m16\u001b[39m },\n",
      "  system_fingerprint: \u001b[1mnull\u001b[22m\n",
      "}\n",
      "Hello! How are you today?\n"
     ]
    }
   ],
   "source": [
    "const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "    messages: [{ role: 'user', content: 'Hello World' }],    \n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(chatCompletion);\n",
    "console.log(chatCompletion.choices[0].message.content);\n",
    "\n",
    "// https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716e929-5d96-40f7-903d-0aa464c5eb89",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ccc259f-e7c0-4c81-bcce-199add9c2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "const streamOutput = async (completion) => {\n",
    "  for await (const chunk of completion) {\n",
    "    console.log(chunk.choices[0].delta.content);\n",
    "  }\n",
    "}\n",
    "\n",
    "const extractMsgFromChatCompletion = (chatCompletion) => {\n",
    "    return chatCompletion.choices[0]?.message?.content || 'null';\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7d048-0ee3-48c1-9bfb-4e0200b66b31",
   "metadata": {},
   "source": [
    "## Message Roles\n",
    "\n",
    "Messages often adopt specific roles to guide the model’s responses. \n",
    "- The “system” provides high-level instructions. you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation.\n",
    "- the “user” presents queries or prompts\n",
    "- the “assistant” is the model’s response.\n",
    "\n",
    "By differentiating these roles, we can set the context and direct the conversation efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "402e0a5e-7663-43e7-bbc9-e66f76b78b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  role: \u001b[32m\"assistant\"\u001b[39m,\n",
      "  content: \u001b[32m\"Why did the scarecrow win an award? Because he was outstanding in his field!\"\u001b[39m,\n",
      "  refusal: \u001b[1mnull\u001b[22m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "    messages: [{ role: 'user', content: 'Tell me a joke' }],\n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(chatCompletion.choices[0].message);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffa1576f-aeaa-4eb7-a6ec-f7663c04d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do JavaScript developers wear glasses?\n",
      "\n",
      "Because they don't C#!\n"
     ]
    }
   ],
   "source": [
    "const chatCompletion_system = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'system',\n",
    "        content: 'You are an assistant for a Frontend Team. Only respond in a way which makes sense to this audience',\n",
    "      },\n",
    "      { role: 'user', content: 'Tell me a joke' },\n",
    "    ],\n",
    "    //model: 'gpt-3.5-turbo',\n",
    "    model: 'gpt-4o',\n",
    "  });\n",
    "console.log(extractMsgFromChatCompletion(chatCompletion_system));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1263c-461c-4ed3-ba07-8634e1a2b6a4",
   "metadata": {},
   "source": [
    "Models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5105d-8c90-4d45-a32c-dab3a054565c",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "tools like ChatGPT are trained to predict strings of words that best match your query. \n",
    "They lack the reasoning to apply logic or consider any factual inconsistencies they're giving out.\n",
    "\n",
    "Shortcomings of LLM\n",
    " - not trained on your personal data\n",
    " - context window is finite\n",
    " - fine-tuning is costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea159b81-02cd-4800-a883-78494d368969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AsyncDropdown\n"
     ]
    }
   ],
   "source": [
    "const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'user',\n",
    "        content: `which component can I use from Spaceweb react library to render a Dropdown which fetches the options from an external API.\n",
    "        Give me name of component directly.`,\n",
    "      },\n",
    "    ],\n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(extractMsgFromChatCompletion(chatCompletion));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6b824-cbe2-479f-bcec-7a1729837aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296700a3-ac96-4d4e-80a8-8677c7aab709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b69aea-fc7d-4009-bce3-f932eda28596",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "- **Retrieve** most relevant data\n",
    "- **Augment** query with context\n",
    "- **Generate** response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc3a57cf-7d05-4391-8dc1-fc4c584717b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AsyncSelect\n"
     ]
    }
   ],
   "source": [
    "const rag_context = `\n",
    "All possible components present in Spaceweb are:\n",
    "\n",
    "Input: An input enables a person to type in text information.,\n",
    "Button: Buttons allow users to take actions, and make choices, with a single tap.\n",
    "Select: The select component allow the user to select an option or options.\n",
    "AsyncSelect: Same as Select component but allows loading options from a remote source.\n",
    "`\n",
    "\n",
    "const rag_query = `which component can I use from Spaceweb react library to render a Dropdown which fetches the options from an external API.\n",
    "        Give me name of component directly`\n",
    "\n",
    "const rag_prompt = `\n",
    "Context information is below.\n",
    "---------------------\n",
    "${rag_context}\n",
    "---------------------\n",
    "Given the context information, answer the query.\n",
    "Query: ${rag_query}\n",
    "Answer:\n",
    "`\n",
    "\n",
    "const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'user',\n",
    "        content: rag_prompt,\n",
    "      },\n",
    "    ],\n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(extractMsgFromChatCompletion(chatCompletion));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f532f-a841-48d1-b1d3-6563329c3c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38f7ad01-f548-47aa-9f09-1cc567ad05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Global LLM Application Development Discussion\n",
      "Most suitable time: 11:00 AM\n"
     ]
    }
   ],
   "source": [
    "const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'user',\n",
    "        content: `I am planning to host a technical discussion about LLM aplication development with a team whose members are located in different geographies.\n",
    "          Locations are India, UAE and Singapore. Tell me a short title for this talk and most suitable time to host this talk in working hour.\n",
    "          No need to explain the answer. Just give me these 2 details. Provide answer in Dubai time`,\n",
    "      },\n",
    "    ],\n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(extractMsgFromChatCompletion(chatCompletion));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6fe60-9881-49d8-91b7-ee1f0f1651b6",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "Do more than text generation. \n",
    "Reliably obtain structured data from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7788cbb7-17f6-474b-aa93-7527b94e53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Given a meeting title and time, send an invite to all attendees.\n",
    "const schedule_meeting = ({ meeting_title, most_suitable_time }) => {\n",
    "  \n",
    "    console.log(`Sending a meeting invite...\n",
    "      Time: ${most_suitable_time}\n",
    "      Title: ${meeting_title}\n",
    "    `);\n",
    "    \n",
    "    console.log('Invite sent ✅');\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d9401d0-aa2f-4a7f-ac21-41ecb7005d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending a meeting invite...\n",
      "      Time: 4PM IST\n",
      "      Title: Hello world\n",
      "    \n",
      "Invite sent ✅\n"
     ]
    }
   ],
   "source": [
    "schedule_meeting({meeting_title: 'Hello world', most_suitable_time: '4PM IST'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a00aede-7ad2-4ea0-868f-76d1d3b2b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  id: \u001b[32m\"chatcmpl-A6Gba1TwJKh4inQO62wcmaAgOXvHb\"\u001b[39m,\n",
      "  object: \u001b[32m\"chat.completion\"\u001b[39m,\n",
      "  created: \u001b[33m1726057162\u001b[39m,\n",
      "  model: \u001b[32m\"gpt-3.5-turbo-0125\"\u001b[39m,\n",
      "  choices: [\n",
      "    {\n",
      "      index: \u001b[33m0\u001b[39m,\n",
      "      message: {\n",
      "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
      "        content: \u001b[1mnull\u001b[22m,\n",
      "        function_call: {\n",
      "          name: \u001b[32m\"schedule_meeting\"\u001b[39m,\n",
      "          arguments: \u001b[32m'{\"meeting_title\":\"LLM Application Development Discussion\",\"most_suitable_time\":\"1636034400\"}'\u001b[39m\n",
      "        },\n",
      "        refusal: \u001b[1mnull\u001b[22m\n",
      "      },\n",
      "      logprobs: \u001b[1mnull\u001b[22m,\n",
      "      finish_reason: \u001b[32m\"function_call\"\u001b[39m\n",
      "    }\n",
      "  ],\n",
      "  usage: { prompt_tokens: \u001b[33m132\u001b[39m, completion_tokens: \u001b[33m30\u001b[39m, total_tokens: \u001b[33m162\u001b[39m },\n",
      "  system_fingerprint: \u001b[1mnull\u001b[22m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const schedule_meeting_definition = {\n",
    "    type: 'function',\n",
    "    name: 'schedule_meeting',\n",
    "    description: 'Schedule a meeting with a team',\n",
    "    parameters: {\n",
    "      type: 'object',\n",
    "      properties: {\n",
    "        meeting_title: {\n",
    "          type: 'string',\n",
    "          description: 'Title of the Meeting',\n",
    "        },\n",
    "        most_suitable_time: {\n",
    "          type: 'string',\n",
    "          description: 'Time of the meeting in Epoch time only.',\n",
    "        },\n",
    "      },\n",
    "      required: ['meeting_title', 'most_suitable_time'],\n",
    "    },\n",
    "};\n",
    "\n",
    "const chatCompletionWithFunction = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'user',\n",
    "        content: `I am planning to host a technical discussion about LLM aplication development with a team whose members are located in Different geographies.\n",
    "          Locations are India, UAE and Singapore. Tell me a short title for this talk and most suitable time to host this talk in working hour.\n",
    "          Provide answer in Dubai time`,\n",
    "      },\n",
    "    ],\n",
    "    functions: [schedule_meeting_definition],\n",
    "    model: 'gpt-3.5-turbo',\n",
    "  });\n",
    "console.log(chatCompletionWithFunction);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "262162d5-278f-49a2-bf91-c66fdd1fa528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending a meeting invite...\n",
      "      Time: 10:00\n",
      "      Title: LLM Application Development Technical Discussion\n",
      "    \n",
      "Invite sent ✅\n"
     ]
    }
   ],
   "source": [
    "const function_call = chatCompletionWithFunction.choices[0]?.message.function_call;\n",
    "\n",
    "if(function_call) {\n",
    "    const function_arguments = JSON.parse(function_call.arguments);\n",
    "    schedule_meeting(function_arguments);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f83138-cadc-447b-89f3-08a592d9be6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11eb1a07-6933-4e1b-a8c0-0b76ebd2cdd7",
   "metadata": {},
   "source": [
    "With function calling, developers can:\n",
    "\n",
    "- Extract structured data from text, making it easier to process and analyze.\n",
    "- Create chatbots that answer questions by invoking external tools, same as ChatGPT Plugins.\n",
    "- Convert natural language inputs into specific API calls or even database queries.\n",
    "\n",
    "\n",
    "When passed multiple functions, model detects when a specific function needs to be invoked based on the user’s input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3d045-accd-40ac-bf9d-c1fad86f0539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e853b46-a9f1-40d1-b2ca-dfe252644ddc",
   "metadata": {},
   "source": [
    "# Tools & Agents\n",
    "\n",
    "## Tools\n",
    "LLMs have incredible text generation capabilities but they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).\n",
    "\n",
    "*Tools* can be thought of as programs you give to a model which can be run as and when the model deems applicable.\n",
    "\n",
    "\n",
    "## Agents\n",
    "Agents allow models to execute multiple steps (i.e. tools) in a non-deterministic way, making decisions based on context and user input.\n",
    "Agents use LLMs to choose the next step in a problem-solving process. They can reason at each step and make decisions based on the evolving context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63c38b-4169-4e38-8ead-42ae48fe9677",
   "metadata": {},
   "source": [
    "# Frontend Copilot\n",
    "\n",
    "- get links for different services\n",
    "- search from Wiki\n",
    "- fetch from graylog and explain why slow\n",
    "- where is this env hosted and deployed?\n",
    "- getErrorBy id on sentry\n",
    "- build a UI\n",
    "- give me link of \"THAT\" sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3f159c7-bfc3-4fab-89fe-ab468b210d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "const getEnvLinks = ({environmentName = 'qa6', linkType}) => {\n",
    "    const getJenkinsJobURL = () => `https://${environmentName}-build.sprinklr.com/jenkins/job`;\n",
    "    const getGralylogURL = () => `https://${environmentName}-logs.sprinklr.com/`;\n",
    "    let response = '';\n",
    "\n",
    "    if(linkType === 'build') {\n",
    "        response = getJenkinsJobURL(environmentName);\n",
    "    } else if(linkType === 'logs') {\n",
    "        response = getGralylogURL(environmentName);\n",
    "    }\n",
    "\n",
    "    return response;\n",
    "};\n",
    "\n",
    "\n",
    "\n",
    "const searchFrontendWiki = async ({query}) => {\n",
    "    //https://sprinklr.atlassian.net/wiki/spaces/Frontend/pages/4235919449/Using+secure+media+in+Applications \n",
    "    const retrivedContext = `\n",
    "        To manage and store static assets used in the code, \n",
    "        utilize the designated folder on the production bucket at \"sprcdn.sprinklr.com/ui/common/assets/\" \n",
    "        This location should consistently be used for serving static assets.\n",
    "\n",
    "        For new uploads, please create an ITOPS ticket to upload to the specified public bucket via this link: ITOPS-698869o Do\n",
    "    `;\n",
    "\n",
    "    const rag_prompt = `\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        ${retrivedContext}\n",
    "        ---------------------\n",
    "        Given the context information, answer the query.\n",
    "        Query: ${query}\n",
    "        Answer:\n",
    "`\n",
    "\n",
    "    const chatCompletion = await _OpenAI.chat.completions.create({\n",
    "        messages: [{role: 'user',content: rag_prompt}],\n",
    "        model: 'gpt-4o',\n",
    "    });\n",
    "    \n",
    "    return extractMsgFromChatCompletion(chatCompletion);\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d2278eb-30c1-4937-95fd-07e59ea50228",
   "metadata": {},
   "outputs": [],
   "source": [
    "const getEnvLinks_definition = {\n",
    "    type: 'function',\n",
    "    function: {\n",
    "        name: 'getEnvLinks',    \n",
    "        description: 'Get links to specific environments',\n",
    "        parameters: {\n",
    "          type: 'object',\n",
    "          properties: {\n",
    "            environmentName: {\n",
    "              type: 'string',\n",
    "              description: 'Name of the environment',\n",
    "            },\n",
    "            linkType: {\n",
    "              type: 'string',\n",
    "              description: 'Type of link to fetch',\n",
    "            },\n",
    "          },\n",
    "          required: ['environmentName', 'linkType'],\n",
    "        },\n",
    "    },            \n",
    "};\n",
    "\n",
    "const searchFrontendWiki_definition = {\n",
    "    type: 'function',\n",
    "    function: {\n",
    "        name: 'searchFrontendWiki',\n",
    "        description: 'Search about a topic from the frontend wiki',\n",
    "        parameters: {\n",
    "            type: 'object',\n",
    "            properties: {\n",
    "                query: {\n",
    "                    type: 'string',\n",
    "                    description: 'Query asked by the user'\n",
    "                }    \n",
    "            },\n",
    "            required: ['query']\n",
    "        },\n",
    "    },    \n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32865ddd-b7b1-4823-baa5-ddff28d1eada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Why did the scarecrow become a successful software developer?\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"Because he was outstanding in his fie\"\u001b[39m... 8 more characters"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Execute Tool Call to LLM:\n",
    "const chatCompletionWithFunction2 = await _OpenAI.chat.completions.create({\n",
    "    messages: [\n",
    "      {\n",
    "        role: 'system',\n",
    "        content: `You are a skilled techinical assistant for a frontend team.`,\n",
    "      },\n",
    "      {\n",
    "        role: 'user',\n",
    "        //content: `Schedule a meeting for 2PM IST for discussion on secure media usage`,\n",
    "        //content: `Give me build link for prod0`,\n",
    "        //content: `Give me logs link for prod`,\n",
    "        content: `Tell me a joke`,\n",
    "        //content: `How do I store URL of static assets in code?`,  \n",
    "      },\n",
    "    ],\n",
    "    //schedule_meeting_definition,\n",
    "    tools: [ getEnvLinks_definition, searchFrontendWiki_definition],\n",
    "    model: 'gpt-4o',\n",
    "    //tool_choice: \"required\"\n",
    "  });\n",
    "\n",
    "const tool_call_response = chatCompletionWithFunction2.choices[0]?.message.tool_calls?.[0] || '';\n",
    "\n",
    "\n",
    "// Execute Tool:\n",
    "const TOOLS = {\n",
    "    schedule_meeting,\n",
    "    getEnvLinks,\n",
    "    searchFrontendWiki,\n",
    "    ///....manymore,\n",
    "};\n",
    "\n",
    "if(tool_call_response) {\n",
    "    const toolToInvoke = TOOLS[tool_call_response.function.name];\n",
    "    const params = tool_call_response.function.arguments;\n",
    "    console.log(`Calling Tool: ${tool_call_response.function.name}. Params: ${tool_call_response.function.arguments}`, '\\n');\n",
    "    \n",
    "    const toolResponse = toolToInvoke && await toolToInvoke(JSON.parse(params));\n",
    "    console.log(toolResponse.toString())\n",
    "} else {\n",
    "    chatCompletionWithFunction2.choices[0]?.message.content\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbf969-7919-445c-8201-4b9806ab40c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399966b7-4193-4723-912d-7ceee473127e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98c08c40-9e43-4736-8be4-59971a8082ae",
   "metadata": {},
   "source": [
    "# SpacewebForge Architecture\n",
    "<img src=\"ss_2.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1872ecd-c1c8-4be4-9efe-59a74a84c97a",
   "metadata": {},
   "source": [
    "# Embeddings and Search\n",
    "\n",
    "Embeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space. In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.\n",
    "\n",
    "The process of calculating the similarity between two vectors is called ‘cosine similarity’ where a value of 1 would indicate high similarity and a value of -1 would indicate high opposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e66e298-3fc9-4eab-a86e-93fce55f4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "//from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "//import {OpenAI} from \"npm:openai\";\n",
    "//OpenAI.embeddings_utils\n",
    "\n",
    "// Function to calculate cosine similarity\n",
    "function cosineSimilarity(vecA, vecB) {\n",
    "    const dotProduct = vecA.reduce((sum, a, idx) => sum + a * vecB[idx], 0);\n",
    "    const magnitudeA = Math.sqrt(vecA.reduce((sum, val) => sum + val * val, 0));\n",
    "    const magnitudeB = Math.sqrt(vecB.reduce((sum, val) => sum + val * val, 0));\n",
    "    return dotProduct / (magnitudeA * magnitudeB);\n",
    "}\n",
    "\n",
    "async function getEmbedding(text) {\n",
    "    const embeddingResponse = await _OpenAI.embeddings.create({\n",
    "        model: \"text-embedding-3-small\",\n",
    "        input: text,\n",
    "        encoding_format: \"float\",\n",
    "    });\n",
    "    return embeddingResponse.data[0].embedding;    \n",
    "}\n",
    "\n",
    "async function semanticSearch(query, documents) {\n",
    "    const queryEmbedding = await getEmbedding(query);\n",
    "    const documentEmbeddings = await Promise.all(documents.map(doc => getEmbedding(doc)));\n",
    "\n",
    "    const similarities = documentEmbeddings.map((embedding, idx) => ({\n",
    "        document: documents[idx],\n",
    "        similarity: cosineSimilarity(queryEmbedding, embedding)\n",
    "    }));\n",
    "\n",
    "    similarities.sort((a, b) => b.similarity - a.similarity);\n",
    "\n",
    "    return similarities;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2275895f-c5b1-483a-b648-d9b0862b67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "     \u001b[33m0.02552942\u001b[39m,  \u001b[33m-0.023411665\u001b[39m, \u001b[33m-0.016092611\u001b[39m,    \u001b[33m0.03937628\u001b[39m,   \u001b[33m0.02094483\u001b[39m,\n",
      "    \u001b[33m-0.02632067\u001b[39m,  \u001b[33m0.0018908527\u001b[39m,  \u001b[33m0.030602723\u001b[39m,  \u001b[33m-0.015929706\u001b[39m, \u001b[33m0.0053118416\u001b[39m,\n",
      "     \u001b[33m0.02214334\u001b[39m, \u001b[33m-0.0002121755\u001b[39m,  \u001b[33m0.010460779\u001b[39m,  \u001b[33m0.0031213614\u001b[39m,   \u001b[33m0.02985802\u001b[39m,\n",
      "    \u001b[33m0.006265995\u001b[39m,  \u001b[33m-0.021363726\u001b[39m, \u001b[33m-0.010716772\u001b[39m,  \u001b[33m-0.030532908\u001b[39m,  \u001b[33m0.057528466\u001b[39m,\n",
      "     \u001b[33m0.03409353\u001b[39m,    \u001b[33m0.04589245\u001b[39m,  \u001b[33m0.020502662\u001b[39m,  \u001b[33m-0.046637155\u001b[39m, \u001b[33m-0.006871068\u001b[39m,\n",
      "     \u001b[33m0.03800323\u001b[39m,  \u001b[33m-0.009268087\u001b[39m,   \u001b[33m0.04405396\u001b[39m,   \u001b[33m0.051803548\u001b[39m, \u001b[33m-0.013497779\u001b[39m,\n",
      "   \u001b[33m0.0033686268\u001b[39m,  \u001b[33m-0.043123078\u001b[39m,   \u001b[33m-0.0112753\u001b[39m,  \u001b[33m-0.029090041\u001b[39m, \u001b[33m-0.022946225\u001b[39m,\n",
      "    \u001b[33m0.017768197\u001b[39m,   \u001b[33m0.017570386\u001b[39m, \u001b[33m-0.028019529\u001b[39m,  \u001b[33m-0.015743531\u001b[39m,   \u001b[33m0.01378868\u001b[39m,\n",
      "   \u001b[33m-0.037281796\u001b[39m,  \u001b[33m-0.008773557\u001b[39m,  \u001b[33m0.045799363\u001b[39m,   \u001b[33m0.011473113\u001b[39m,  \u001b[33m0.009460081\u001b[39m,\n",
      "     \u001b[33m-0.0533395\u001b[39m,  \u001b[33m-0.022597145\u001b[39m, \u001b[33m-0.019606689\u001b[39m,   \u001b[33m0.019362332\u001b[39m,  \u001b[33m0.037142165\u001b[39m,\n",
      "    \u001b[33m0.023388393\u001b[39m,  \u001b[33m-0.014870829\u001b[39m,   \u001b[33m0.01746566\u001b[39m,    \u001b[33m0.04998833\u001b[39m, \u001b[33m-0.004168603\u001b[39m,\n",
      "  \u001b[33m-0.0011636016\u001b[39m,  \u001b[33m-0.019292515\u001b[39m,   \u001b[33m0.04659061\u001b[39m, \u001b[33m-0.0029279126\u001b[39m,  \u001b[33m0.009279723\u001b[39m,\n",
      "   \u001b[33m-0.024970891\u001b[39m,  \u001b[33m0.0059925485\u001b[39m,   \u001b[33m0.02518034\u001b[39m,  \u001b[33m-0.002679193\u001b[39m,  \u001b[33m0.019420512\u001b[39m,\n",
      "    \u001b[33m0.038282495\u001b[39m,    \u001b[33m0.01837327\u001b[39m,  \u001b[33m0.017232941\u001b[39m,   \u001b[33m-0.05962295\u001b[39m, \u001b[33m-0.018210366\u001b[39m,\n",
      "  \u001b[33m-0.0058034635\u001b[39m,   \u001b[33m0.028415153\u001b[39m, \u001b[33m-0.062089786\u001b[39m,   \u001b[33m0.011286936\u001b[39m,  \u001b[33m0.047218956\u001b[39m,\n",
      "    \u001b[33m0.009401902\u001b[39m,  \u001b[33m-0.029974379\u001b[39m, \u001b[33m-0.000250538\u001b[39m,   \u001b[33m0.062974125\u001b[39m,  \u001b[33m0.043425616\u001b[39m,\n",
      "   \u001b[33m0.0011352389\u001b[39m,   \u001b[33m0.058552437\u001b[39m,  \u001b[33m0.016243879\u001b[39m,  \u001b[33m-0.025226884\u001b[39m,   \u001b[33m0.01259017\u001b[39m,\n",
      "   \u001b[33m-0.023202218\u001b[39m,  \u001b[33m-0.034512427\u001b[39m,   \u001b[33m0.02850824\u001b[39m,   \u001b[33m0.011054216\u001b[39m, \u001b[33m-0.026041405\u001b[39m,\n",
      "  \u001b[33m-0.0038457036\u001b[39m,   \u001b[33m0.015487539\u001b[39m, \u001b[33m-0.044798665\u001b[39m,  \u001b[33m-0.038980655\u001b[39m, \u001b[33m-0.010332783\u001b[39m,\n",
      "    \u001b[33m0.043774694\u001b[39m,  \u001b[33m-0.008517564\u001b[39m, \u001b[33m-0.048219655\u001b[39m,  \u001b[33m-0.001969396\u001b[39m,  \u001b[33m0.014149397\u001b[39m,\n",
      "  ... 1436 more items\n",
      "]\n",
      "dimensions: \u001b[33m1536\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "const cat_embedding = await getEmbedding('cat');\n",
    "\n",
    "console.log(cat_embedding);\n",
    "console.log(\"dimensions:\", cat_embedding.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ef840b8-f24c-403c-a635-fd83479e5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Document: \"Dogs are great pets.\", Similarity: 0.6798705258123072\n",
      "Document: \"The cat sat on the mat.\", Similarity: 0.3102847013851084\n",
      "Document: \"The quick brown fox jumps over the lazy dog.\", Similarity: 0.26688707103942033\n",
      "Document: \"A journey of a thousand miles begins with a single step.\", Similarity: 0.16938206733128902\n"
     ]
    }
   ],
   "source": [
    "const documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\"\n",
    "];\n",
    "        \n",
    "const query = \"Pets are wonderful companions.\";\n",
    "\n",
    "const results = await semanticSearch(query, documents);\n",
    "\n",
    "console.log(\"Results:\");\n",
    "results.forEach(result => {\n",
    "    console.log(`Document: \"${result.document}\", Similarity: ${result.similarity}`);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c7320-46ba-4278-b741-4becae66bd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf7f197c-be53-4a71-aa55-3f7d16e2a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Document: \"Birds\", Similarity: 0.4872772923435113\n",
      "Document: \"Animals\", Similarity: 0.3539369319183071\n"
     ]
    }
   ],
   "source": [
    "const documents = [\n",
    "    \"Animals\",\n",
    "    \"Birds\",    \n",
    "];\n",
    "        \n",
    "const query = \"Peacock\";\n",
    "\n",
    "const results = await semanticSearch(query, documents);\n",
    "\n",
    "console.log(\"Results:\");\n",
    "results.forEach(result => {\n",
    "    console.log(`Document: \"${result.document}\", Similarity: ${result.similarity}`);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be373e3-08aa-4e87-9708-3133650cef6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c970c2aa-cbff-4004-bc5a-b3b14d083726",
   "metadata": {},
   "source": [
    "# Further Readings and References\n",
    "- [OpenAI Prompt engineering guidelines](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results)\n",
    "- Courses\n",
    "    - [https://learn.deeplearning.ai/](https://learn.deeplearning.ai/)\n",
    "- Podcast\n",
    "    - [Aravind Srinivas - Lex Fridman](https://www.youtube.com/watch?v=e-gwvmhyU7A)\n",
    "    - [Andrej Karpathy - Lex Fridman](https://www.youtube.com/watch?v=cdiD-9MMpb0)\n",
    "- Frameworks\n",
    "    - [Vercel AI SDK](https://sdk.vercel.ai/)\n",
    "    - [LangChain](https://js.langchain.com/v0.2/docs/tutorials/)\n",
    "    - [Llama Index](https://www.llamaindex.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bec7c5-f3bd-41b9-9cc1-84c181974a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
